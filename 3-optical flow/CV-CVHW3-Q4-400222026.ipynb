{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"id":"leTYxqjIgEuA","execution":{"iopub.status.busy":"2024-06-16T19:30:40.987702Z","iopub.execute_input":"2024-06-16T19:30:40.988316Z","iopub.status.idle":"2024-06-16T19:30:40.993729Z","shell.execute_reply.started":"2024-06-16T19:30:40.988282Z","shell.execute_reply":"2024-06-16T19:30:40.992546Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nbatch_size = 64\n\n# CIFAR-10 dataset preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load CIFAR-10 dataset\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# Data loaders\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hf_qf6VBgnhS","outputId":"7a30992b-f1bb-4fe5-d639-2f4e8d2fb052","execution":{"iopub.status.busy":"2024-06-16T19:30:41.040340Z","iopub.execute_input":"2024-06-16T19:30:41.040675Z","iopub.status.idle":"2024-06-16T19:30:42.668912Z","shell.execute_reply.started":"2024-06-16T19:30:41.040648Z","shell.execute_reply":"2024-06-16T19:30:42.667907Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"class MultiheadAttentionEinsum(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super(MultiheadAttentionEinsum, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.head_dim = embedding_dim // num_heads\n\n        assert self.head_dim * num_heads == embedding_dim, \"embedding_dim must be divisible by num_heads\"\n\n        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        # Linear projections\n        Q = self.q_linear(query)\n        K = self.k_linear(key)\n        V = self.v_linear(value)\n\n        # Reshape and permute for multi-head attention\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        # Scaled dot-product attention\n        energy = torch.einsum(\"bnqd,bnkd->bnqk\", Q, K)\n        scaling_factor = self.head_dim ** 0.5\n        scaled_energy = energy / scaling_factor\n        attention = F.softmax(scaled_energy, dim=-1)\n\n        # Attention values\n        attended_values = torch.einsum(\"bnqk,bnvd->bnqd\", attention, V)\n\n        # Concatenate heads and put through final linear layer\n        attended_values = attended_values.permute(0, 2, 1, 3).contiguous()\n        attended_values = attended_values.view(batch_size, -1, self.embedding_dim)\n        out = self.fc_out(attended_values)\n\n        return out\n    \nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super(TransformerEncoderLayer, self).__init__()\n        self.multihead_attention = MultiheadAttentionEinsum(embed_dim=embedding_dim, num_heads=num_heads)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embedding_dim, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, embedding_dim)\n        )\n        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n\n    def forward(self, x):\n        residual = x\n        x = self.layer_norm1(x)\n        x = x.permute(1, 0, 2)  # (seq_len, batch_size, embedding_dim)\n        attn_output = self.multihead_attention(x, x, x)[0]  # self-attention\n        x = attn_output + residual\n        x = x.permute(1, 0, 2)  # (batch_size, seq_len, embedding_dim)\n\n        residual = x\n        x = self.layer_norm2(x)\n        x = self.feed_forward(x)\n        x = x + residual\n\n        return x\n    \nclass VisionTransformer(nn.Module):\n    def __init__(self, num_classes, patch_size, embedding_dim, num_heads, num_layers):\n        super(VisionTransformer, self).__init__()\n        self.patch_embedding = nn.Conv2d(3, embedding_dim, kernel_size=patch_size, stride=patch_size)\n        self.positional_encoding = nn.Parameter(torch.randn(1, 14 * 14 + 1, embedding_dim))\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n        ])\n        self.fc = nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.patch_embedding(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = torch.cat((x, self.positional_encoding.repeat(batch_size, 1, 1)), dim=1)\n        for layer in self.transformer_layers:\n            x = layer(x)\n        x = x.mean(dim=1)\n        x = self.fc(x)\n        return x","metadata":{"id":"HuZiPWITgcl-","execution":{"iopub.status.busy":"2024-06-16T19:30:42.670859Z","iopub.execute_input":"2024-06-16T19:30:42.671152Z","iopub.status.idle":"2024-06-16T19:30:42.691271Z","shell.execute_reply.started":"2024-06-16T19:30:42.671127Z","shell.execute_reply":"2024-06-16T19:30:42.690291Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_epochs = 10\nlearning_rate = 0.001\nnum_classes = 10\npatch_size = 16\nembedding_dim = 128\nnum_heads = 8\nnum_layers = 3\n\nmodel = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nlosses=[]\n# Training loop\ntotal_steps = len(train_loader)\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n\n        if (i + 1) % 100 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"Q1iPe3XBg8_j","outputId":"e1dac23b-3e4d-4f6f-f8b7-8eb032c4abe5","execution":{"iopub.status.busy":"2024-06-16T19:30:42.692208Z","iopub.execute_input":"2024-06-16T19:30:42.692497Z","iopub.status.idle":"2024-06-16T19:57:56.101342Z","shell.execute_reply.started":"2024-06-16T19:30:42.692472Z","shell.execute_reply":"2024-06-16T19:57:56.100517Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [100/782], Loss: 1.9912\nEpoch [1/10], Step [200/782], Loss: 2.1571\nEpoch [1/10], Step [300/782], Loss: 2.2335\nEpoch [1/10], Step [400/782], Loss: 2.2162\nEpoch [1/10], Step [500/782], Loss: 2.1070\nEpoch [1/10], Step [600/782], Loss: 2.1525\nEpoch [1/10], Step [700/782], Loss: 2.0390\nEpoch [2/10], Step [100/782], Loss: 2.2897\nEpoch [2/10], Step [200/782], Loss: 2.2710\nEpoch [2/10], Step [300/782], Loss: 2.0789\nEpoch [2/10], Step [400/782], Loss: 2.3112\nEpoch [2/10], Step [500/782], Loss: 2.2923\nEpoch [2/10], Step [600/782], Loss: 2.2128\nEpoch [2/10], Step [700/782], Loss: 2.1280\nEpoch [3/10], Step [100/782], Loss: 2.0410\nEpoch [3/10], Step [200/782], Loss: 2.1790\nEpoch [3/10], Step [300/782], Loss: 2.0591\nEpoch [3/10], Step [400/782], Loss: 2.2117\nEpoch [3/10], Step [500/782], Loss: 2.1006\nEpoch [3/10], Step [600/782], Loss: 2.1199\nEpoch [3/10], Step [700/782], Loss: 2.2086\nEpoch [4/10], Step [100/782], Loss: 2.1662\nEpoch [4/10], Step [200/782], Loss: 2.1268\nEpoch [4/10], Step [300/782], Loss: 2.1672\nEpoch [4/10], Step [400/782], Loss: 2.1313\nEpoch [4/10], Step [500/782], Loss: 2.2095\nEpoch [4/10], Step [600/782], Loss: 2.1448\nEpoch [4/10], Step [700/782], Loss: 2.1364\nEpoch [5/10], Step [100/782], Loss: 2.2601\nEpoch [5/10], Step [200/782], Loss: 2.1898\nEpoch [5/10], Step [300/782], Loss: 2.1977\nEpoch [5/10], Step [400/782], Loss: 2.0362\nEpoch [5/10], Step [500/782], Loss: 2.1189\nEpoch [5/10], Step [600/782], Loss: 2.0739\nEpoch [5/10], Step [700/782], Loss: 2.2115\nEpoch [6/10], Step [100/782], Loss: 2.1848\nEpoch [6/10], Step [200/782], Loss: 2.2136\nEpoch [6/10], Step [300/782], Loss: 2.0459\nEpoch [6/10], Step [400/782], Loss: 2.1092\nEpoch [6/10], Step [500/782], Loss: 2.1572\nEpoch [6/10], Step [600/782], Loss: 1.9099\nEpoch [6/10], Step [700/782], Loss: 2.0987\nEpoch [7/10], Step [100/782], Loss: 2.1547\nEpoch [7/10], Step [200/782], Loss: 2.2107\nEpoch [7/10], Step [300/782], Loss: 2.1600\nEpoch [7/10], Step [400/782], Loss: 2.1352\nEpoch [7/10], Step [500/782], Loss: 2.1094\nEpoch [7/10], Step [600/782], Loss: 2.1136\nEpoch [7/10], Step [700/782], Loss: 2.2043\nEpoch [8/10], Step [100/782], Loss: 2.2279\nEpoch [8/10], Step [200/782], Loss: 2.2311\nEpoch [8/10], Step [300/782], Loss: 2.1194\nEpoch [8/10], Step [400/782], Loss: 2.1049\nEpoch [8/10], Step [500/782], Loss: 2.1126\nEpoch [8/10], Step [600/782], Loss: 2.1443\nEpoch [8/10], Step [700/782], Loss: 2.0121\nEpoch [9/10], Step [100/782], Loss: 2.0531\nEpoch [9/10], Step [200/782], Loss: 2.1702\nEpoch [9/10], Step [300/782], Loss: 2.1950\nEpoch [9/10], Step [400/782], Loss: 2.1198\nEpoch [9/10], Step [500/782], Loss: 2.0258\nEpoch [9/10], Step [600/782], Loss: 2.1181\nEpoch [9/10], Step [700/782], Loss: 2.1550\nEpoch [10/10], Step [100/782], Loss: 2.1791\nEpoch [10/10], Step [200/782], Loss: 2.0916\nEpoch [10/10], Step [300/782], Loss: 2.1138\nEpoch [10/10], Step [400/782], Loss: 2.2534\nEpoch [10/10], Step [500/782], Loss: 2.0873\nEpoch [10/10], Step [600/782], Loss: 2.2121\nEpoch [10/10], Step [700/782], Loss: 2.1484\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Testing phase\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f}%')","metadata":{"id":"AoG3m2IPhCoH","execution":{"iopub.status.busy":"2024-06-16T19:57:56.103063Z","iopub.execute_input":"2024-06-16T19:57:56.103379Z","iopub.status.idle":"2024-06-16T19:58:15.114047Z","shell.execute_reply.started":"2024-06-16T19:57:56.103343Z","shell.execute_reply":"2024-06-16T19:58:15.113141Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Test Accuracy of the model on the 10000 test images: 19.18%\n","output_type":"stream"}]}]}